---
title: "Session 4"
author: "Matt Denwood"
date: '2023-07-13'
output:
  html_document: default
  beamer_presentation:
    pandoc_args:
    - -t
    - beamer
    slide_level: 2
theme: metropolis
aspectratio: 169
colortheme: seahorse
header-includes: \input{../rsc/preamble}
params:
  presentation: no
subtitle: Model Fit Assessment
---

```{r setup, include=FALSE}
source("../rsc/setup.R")
```


# Parsimony

## Occam's (or Ockham’s) razor

“entia non sunt multiplicanda praeter necessitatem”

[“entities must not be multiplied beyond necessity”]

The simplest explanation that adequately describes the data should be preferred

If we add more and more parameters to a model we (should) always get a better and better fit until the model is saturated

So should we exclude some parameters from the model on the basis that they don’t significantly improve model fit?

In the context of Hui-Walter models, we are usually only interested in testing for inclusion of covariance parameters.

NOTE:  this is a measure of relative fit, not absolute fit!!


## Assessing parsimony

1.  What is the likelihood of the model?

- Usually we use deviance rather than likelihood…
  - deviance = - 2 x logPosterior
  - #monitor# deviance

2. How many parameters are in the model?

- A model with as many parameters as data points is a saturated model


## Frequentist fit statistics

Likelihood ratio test:  does adding a new parameter give us a significantly better fit than expected by chance?

AIC:  Generalisation of LRT to non-nested models

- AIC = 2k – 2ln(L)
- Requires knowing how many parameters (k) are in each model


## A Bayesian k

Consider four stochastic parameters:

1. prev ~ dbeta(1,1) :  Probably 1 parameter

2. prev ~ dbeta(5,5)  :  A fraction of a parameter?
	
3. prev ~ dbeta(5000,5000)  :  Roughly zero parameters?
  
4. prev <- 0.5  :  Definitely zero parameters!


## pD

The ‘effective number of parameters’

Caveats:

- Accurate calculation depends on approximate normality in the posteriors 
    - NOT POSSIBLE WITH MIXTURE MODELS
    
- Not invariant to re-parameterisation

- Requires sample size of data to be much larger than pD

- Sometimes comes out negative….
  - Especially if strong prior/data conflict

- There are multiple ways to calculate pD (and equivalents) with no real consensus on the ‘best’ approach...!


## DIC

"Deviance Information Criterion"

Model deviance:  $D = -2 \ log \ p(y | \theta)$

Posterior mean deviance:  $\bar{D} = E[D]$

$pD =  E[D]$ – deviance evaluated at posterior mean of the parameters

$pD = \bar{D} - D(\bar{\theta})$

DIC = Goodness of fit + complexity:

$DIC = \bar{D} + pD$

Spiegelhalter, D. J., Best, N. G., Carlin, B. P., & Linde, A. van der. (2002). Bayesian Measures of Model Complexity and Fit. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 64(4), 583-639. Blackwell Publishing for the Royal Statistical Society. http://www.jstor.org/stable/3088806


### DIC variants

Original Spiegelhalter et al. definition of pD - used by WinBUGS and OpenBUGS

Plummer (2002) definition of pD - used by rjags and runjags

Gelman et al (2004) definition of pD - easy to calculate from any MCMC output; 
used by r2jags and others

Plummer (2008) definition of penalized expected deviance (PED) - also used by rjags and runjags

So DIC is not always consistent between software!!!


Plummer, M. (2002), Discussion of the paper by Spiegelhalter et al. Journal of the Royal Statistical 	Society Series B 64, 620.

Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2004). Bayesian Data Analysis (2nd ed.). 	Chapman and Hall/CRC.

Plummer, M. (2008) Penalized loss functions for Bayesian model comparison. Biostatistics doi: 	10.1093/biostatistics/kxm049


### Using DIC

```{r}
mod <- "model{
  for(p in 1:P){
    Tally[1:4,p] ~ dmulti(prob[1:4,p], N[p])
  
    prob[1,p] <- (prev[p] * ((1-se[1])*(1-se[2]))) + ((1-prev[p]) * ((sp[1])*(sp[2])))
    prob[2,p] <- (prev[p] * ((se[1])*(1-se[2]))) + ((1-prev[p]) * ((1-sp[1])*(sp[2])))
    prob[3,p] <- (prev[p] * ((1-se[1])*(se[2]))) + ((1-prev[p]) * ((sp[1])*(1-sp[2])))
    prob[4,p] <- (prev[p] * ((se[1])*(se[2]))) + ((1-prev[p]) * ((1-sp[1])*(1-sp[2])))

    prev[p] ~ dbeta(1, 1)
  }

  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Tally, N, P
  #monitor# prev, prob, se, sp, deviance, dic, ped
}"

Tally <- structure(c(205, 22, 26, 47, 142, 39, 31, 88), .Dim = c(4, 2))
N <- apply(Tally,2,sum)
P <- 2L

## NOTE: the default method="rjags" seems to have a bug!
run.jags(mod, n.chains=2, method="interruptible")
```


### Interpretation

Smaller is better:  smaller deviance and/or fewer parameters

Only makes sense as a relative comparison of parameters FOR THE SAME DATA

Rule of thumb:

- A difference in DIC of < 5 is probably meaningless
- A difference in DIC of 5-10 suggests the preferred model
- A difference in DIC of >10 is more conclusive

It is the ABSOLUTE DIFFERENCE that matters:  the specific values and/or ratios are meaningless

Notes:

- Theoretical derivation is based on hierarchical normal models
- Has proven in practice to be more generally useful with GL(M)M ...?
- Utility has NOT been proven in the general case - and there are strong theoretical issues with using it with mixture models
- The model deviance is a Monte Carlo approximation
- DIC can be improved by priors that match the data!!!
- All models are wrong anyway...
- Always look for potentially important differences in posterior inference from competing models
- Always include variables on biological plausibility first


## WAIC

"Widely Applicable Information Criterion"

Similar use/interpretation to DIC but with fewer drawbacks:

- Theory is better understood (approximation to LOO)
- WAIC is valid for singular models e.g. mixture models
- Calculation is based on the mean and variance of the individual data-point contributions to the likelihood


Features/problems:

- Requires the ‘focus’ of interest to be specified explicitly
- Allows more specific ‘tailoring’ of the precise aspect of the model fit that we are assessing
- Also requires an extra bit of thinking


References:

- Vehtari and Gelman, 2014:  WAIC and cross-validation in Stan 
- Vehtari, Gelman and Gaby, 2016:  Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC 


### Using WAIC

We need to specify the log likelihood (density) of interest:

```{r}
mod <- "model{
  for(p in 1:P){
    Tally[1:4,p] ~ dmulti(prob[1:4,p], N[p])
  
    prob[1,p] <- (prev[p] * ((1-se[1])*(1-se[2]))) + ((1-prev[p]) * ((sp[1])*(sp[2])))
    prob[2,p] <- (prev[p] * ((se[1])*(1-se[2]))) + ((1-prev[p]) * ((1-sp[1])*(sp[2])))
    prob[3,p] <- (prev[p] * ((1-se[1])*(se[2]))) + ((1-prev[p]) * ((sp[1])*(1-sp[2])))
    prob[4,p] <- (prev[p] * ((se[1])*(se[2]))) + ((1-prev[p]) * ((1-sp[1])*(1-sp[2])))

    prev[p] ~ dbeta(1, 1)
  }

  se[1] ~ dbeta(1, 1)T(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)T(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Tally, N, P
  #monitor# prev, prob, se, sp, deviance, dic, ped
}"

Tally <- structure(c(205, 22, 26, 47, 142, 39, 31, 88), .Dim = c(4, 2))
N <- apply(Tally,2,sum)
P <- 2L

## NOTE: the default method="rjags" seems to have a bug!
run.jags(mod, n.chains=2, method="interruptible")
```

for(i in 1:N){
	Obs[i] ~ dpois(lambda[i])
	log(lambda[i]) <- ...

	# To monitor the variance of the log likelihood:
	logdens_Obs[i] <- logdensity.pois(Obs[i], lambda[i])
	# And the mean of the likelihood:
	density_Obs[i] <- exp(logdens_Obs[i])
}

Some additional R code is then needed:  see the waic_example.R file

NB:  All distributions have a corresponding logdensity function
But JAGS 5 will remove the requirement for calculating logdens_Obs…








TODO

Zeros trick
Ones trick
Run with no data -> priors

